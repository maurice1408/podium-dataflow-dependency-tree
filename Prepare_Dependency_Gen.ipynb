{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Podium Dataflow Dependency Tree\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This Jupyter Notebook demonstrates how it is possible to generate Podium dataflow / source / entity dependency tree from the Podium metadata database (in this case PostgreSQL) using Python.\n",
    "\n",
    "The starting point for the example output (below) is the name of a dataflow \"prod_stg_compsych_member_t\", this starting point should be the dataflow that you are intrested in determining it's predecessor dataflows.\n",
    "\n",
    "The process will look at the LOADERs in the starting point dataflow and look back up the dependency tree to determine the source of each of the LOADERs. \n",
    "\n",
    "These sources can be a Podium Source / Entity or the output from a prior dataflow. \n",
    "\n",
    "Where the source is the output from a prior dataflow, the process recurses on itself until:\n",
    "\n",
    "* There are only Podium Source / Entities remaining or\n",
    "* The source dataflow is in a stop list of dataflow names provided to the process.\n",
    "\n",
    "The result of the process is a [NetworkX](https://networkx.github.io) Graph where the nodes are dataflows, sources and entities, the edges reflect the relationships.\n",
    "\n",
    "The final Graph is converted to a [DOT](https://en.wikipedia.org/wiki/DOT_(graph_description_language)) file and rendered to the png shown here using the dot.exe command installed with [Graphviz](https://www.graphviz.org/).\n",
    "\n",
    "```bash\n",
    "dot.exe -Tpng -o prod_stg_compsych_member_t.png prod_stg_compsych_member_t.dot\n",
    "```\n",
    "\n",
    "## Sample output\n",
    "\n",
    "![dataflow](legend.png) ![dataflow](prod_stg_compsych_member_t.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Notes\n",
    "\n",
    "Access to the PostgreSQL database is through Python SQLAlchemy, the coding style chosen here for SQLAlchemy was chosen for clarity and simplicity. \n",
    "\n",
    "I am sure that there are many improvements and optimizations that could be made.\n",
    "\n",
    "The Python version used here was 3.6.5 installed from the Anaconda distribution which means that all the packages imported were pre installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy import MetaData , Table, Column, Integer, Numeric, String, DateTime, ForeignKey, Text\n",
    "from sqlalchemy import select, desc\n",
    "from sqlalchemy import and_, or_, not_\n",
    "from sqlalchemy import text\n",
    "from sqlalchemy.sql import func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import json\n",
    "import datetime as dt\n",
    "import os\n",
    "import yaml\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydot\n",
    "from networkx.drawing.nx_pydot import write_dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Podium Database Connection\n",
    "\n",
    "The Podium repository database connection details are specified in a yaml format and parsed by the `get_podium_cfg()` finction below.\n",
    "\n",
    "The input to the function is a stream, this can be a string or an open file handle.\n",
    "\n",
    "The expected yaml stream should be:\n",
    "\n",
    "```yaml\n",
    "pd_connect:\n",
    "    user: 'user_name'\n",
    "    password: 'password'\n",
    "    db: 'podium_md'\n",
    "    host: 'hostname'\n",
    "    port: nnnn\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "\n",
    "Most of these functions are helper functions in accessing the Podium metadata database, the function where the actual work is done is the `get_bundle()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_podium_cfg(yaml_stream):\n",
    "    \"\"\"Loads the Podium connection parameters from the input stream\"\"\"\n",
    "    \n",
    "    try:\n",
    "        pd_cfg = yaml.load(yaml_stream)\n",
    "    except:\n",
    "        raise \"Unexpected error reading yaml stream\"\n",
    "        \n",
    "    return pd_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect(user, password, db, host, port):\n",
    "    '''Returns a connection and a metadata object'''\n",
    "    \n",
    "    url = f'postgresql+psycopg2://{user}:{password}@{host}:{port}/{db}'\n",
    "\n",
    "    # The return value of create_engine() is our connection object\n",
    "    con = create_engine(url, client_encoding='utf8')\n",
    "\n",
    "    # We then bind the connection to MetaData()\n",
    "    meta = MetaData(bind=con)\n",
    "\n",
    "    return con, meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_source(con, meta, source_nid):\n",
    "    \"\"\"Retrieve Podium Source row by nid\n",
    "    \n",
    "    Parameters\n",
    "    ==========\n",
    "    \n",
    "    con : SQLAlchemy connection\n",
    "    meta : SQL Alchemy Meta Object\n",
    "    source_nid : Integer source nid\n",
    "    \n",
    "    Returns\n",
    "    =======\n",
    "    \n",
    "    Uses ResultProxy first() to retrieve one row and close the result set.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    assert isinstance(source_nid, int)\n",
    "    \n",
    "    pd_source = Table('podium_core.pd_source', meta)\n",
    "    \n",
    "    s = pd_source.select()\n",
    "    s = s.where(pd_source.c.nid == source_nid)\n",
    "    \n",
    "    r = con.execute(s)\n",
    "    \n",
    "    assert r.rowcount == 1\n",
    "    \n",
    "    return r.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_source_byname(con, meta, source_name, source_type = 'PODIUM_INTERNAL'):\n",
    "    \"\"\"Returns the Podium source row for a named Source\"\"\"\n",
    "\n",
    "    lc_source_name = source_name.lower()\n",
    "    \n",
    "    pd_source = Table('podium_core.pd_source', meta)\n",
    "    \n",
    "    s = select([pd_source.c.nid,\n",
    "                pd_source.c.sname])\n",
    "    \n",
    "    s = s.where(and_(func.lower(pd_source.c.sname) == lc_source_name,\n",
    "                     pd_source.c.source_type == source_type))\n",
    "\n",
    "    rp = con.execute(s)\n",
    "    \n",
    "    return rp.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entity(con, meta, entity_nid):\n",
    "    \"\"\"Fetches the entity record by the entity nid, returns 1 row at most\"\"\"\n",
    "    \n",
    "    assert isinstance(entity_nid, int)\n",
    "    \n",
    "    pd_entity = Table('podium_core.pd_entity', meta)\n",
    "    \n",
    "    s = select([pd_entity.c.sname,\n",
    "                pd_entity.c.source_nid])\n",
    "    \n",
    "    s = s.where(pd_entity.c.nid == entity_nid)\n",
    "    \n",
    "    rp = con.execute(s)\n",
    "    \n",
    "    return rp.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entity_byname(con, meta, source_nid, entity_name):\n",
    "    \"\"\"Fetches the entity record by entity name, returns 1 row at most\"\"\"\n",
    "    \n",
    "    assert isinstance(source_nid, int)\n",
    "    \n",
    "    lc_entity_name = entity_name.lower()\n",
    "    \n",
    "    pd_entity = Table('podium_core.pd_entity', meta)\n",
    "    \n",
    "    s = select([pd_entity.c.nid,\n",
    "                pd_entity.c.sname])\n",
    "    \n",
    "    s = s.where(and_(func.lower(pd_entity.c.sname) == lc_entity_name,\n",
    "                     pd_entity.c.source_nid == source_nid))\n",
    "    \n",
    "    rp = con.execute(s)\n",
    "    \n",
    "    return rp.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entity_store(con, meta, entity_id):\n",
    "    \"\"\"Fetches the dataflows (if any) that STORE the passed entity id\"\"\"\n",
    "    \n",
    "    assert isinstance(entity_id, int)\n",
    "    \n",
    "    pd_prep_package = Table('podium_core.pd_prep_package', meta)\n",
    "    pd_bundle = Table('podium_core.pd_bundle', meta)\n",
    "    \n",
    "    s = select([pd_bundle.c.nid, pd_bundle.c.sname])\n",
    "         \n",
    "    s = s.select_from(pd_prep_package.join(pd_bundle, pd_prep_package.c.bundle_nid == pd_bundle.c.nid))\n",
    "    \n",
    "    s = s.where(and_(pd_prep_package.c.entity_id == entity_id,\n",
    "                     pd_prep_package.c.package_type == 'STORE'))\n",
    "    \n",
    "    rp = con.execute(s, eid=entity_id).fetchall()\n",
    "    \n",
    "    return rp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bundle_id(con, meta, sname):\n",
    "    \"\"\"Get the bundle id of the passed Prepare Workflow name.\n",
    "    \n",
    "    This is a case insensitive match and can only return a single row\n",
    "    or None\n",
    "    \"\"\"\n",
    "    \n",
    "    pd_bundle = Table('podium_core.pd_bundle', meta)\n",
    "    \n",
    "    lc_sname = sname.lower()\n",
    "    \n",
    "    s = pd_bundle.select()\n",
    "    s = s.where(func.lower(pd_bundle.c.sname) == lc_sname)\n",
    "    \n",
    "    rp = con.execute(s)\n",
    "    \n",
    "    r = rp.first()\n",
    "    \n",
    "    return r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bundle_gui_state(con, meta, nid):\n",
    "    \"\"\"Get the bundle gui state record datflow nid.\n",
    "    \n",
    "    \"\"\"\n",
    "    pd_bundle_gui_state = Table('podium_core.pd_bundle_gui_state', meta)\n",
    "    \n",
    "    gui_cols = [pd_bundle_gui_state.c.nid,\n",
    "                pd_bundle_gui_state.c.created_ttz,\n",
    "                pd_bundle_gui_state.c.modified_ttz,\n",
    "                pd_bundle_gui_state.c.version,\n",
    "                pd_bundle_gui_state.c.modifiedby,\n",
    "                pd_bundle_gui_state.c.createdby]\n",
    "    \n",
    "    s = select(gui_cols)\n",
    "    \n",
    "    s = s.where(pd_bundle_gui_state.c.nid == nid)\n",
    "    \n",
    "    rp = con.execute(s)\n",
    "    \n",
    "    return rp.first()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bundle_last_execution(con, meta, bundle_nid, count=10):\n",
    "    \"\"\"Get the last count execution details of the specified bundle.\n",
    "    \n",
    "    \"\"\"\n",
    "    pd_prepare_execution_workorder = Table('podium_core.pd_prepare_execution_workorder', meta)\n",
    "    \n",
    "    wo_cols = [pd_prepare_execution_workorder.c.nid,\n",
    "               pd_prepare_execution_workorder.c.record_count,\n",
    "               pd_prepare_execution_workorder.c.start_time,\n",
    "               pd_prepare_execution_workorder.c.end_time]\n",
    "    \n",
    "    e = select(wo_cols)\n",
    "    \n",
    "    e = e.where(and_(pd_prepare_execution_workorder.c.bundle_nid == bundle_nid, \n",
    "                     pd_prepare_execution_workorder.c.end_time.isnot(None),\n",
    "                     pd_prepare_execution_workorder.c.workorder_status == \"FINISHED\"))\n",
    "    \n",
    "    e = e.order_by(desc(pd_prepare_execution_workorder.c.end_time))\n",
    "    \n",
    "    e = e.limit(count)\n",
    "    \n",
    "    rp = con.execute(e)\n",
    "    \n",
    "    r = rp.fetchall()\n",
    "    \n",
    "    rp.close()\n",
    "    \n",
    "    return r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entity_last_load(con, meta, source_nid, entity_name, n=1):\n",
    "    \"\"\"Get the last execution details of the specified bundle.\n",
    "    \"\"\"\n",
    "    \n",
    "    pd_source = Table('podium_core.pd_source', meta)\n",
    "    pd_entity = Table('podium_core.pd_entity', meta)\n",
    "    pd_workorder = Table('podium_core.pd_workorder', meta)\n",
    "    \n",
    "    #print entity_name\n",
    "    \n",
    "    parent_source = get_source(con, meta, source_nid)\n",
    "    \n",
    "    src = pd_source.select()\n",
    "    src = src.where(pd_source.c.sname == parent_source.sname)\n",
    "    \n",
    "    srp = con.execute(src)\n",
    "    \n",
    "    orig_source_id = None\n",
    "    \n",
    "    for r in srp:\n",
    "        print(f'Source: {r.sname}, Source Type: {r.source_type}, nid: {r.nid}')\n",
    "        if r.source_type != 'PODIUM_INTERNAL':\n",
    "            orig_source_id = r.nid\n",
    "            break\n",
    "    \n",
    "    print(f'orig_source_id: {orig_source_id}')\n",
    "    \n",
    "    if orig_source_id is None:\n",
    "        return None\n",
    "    \n",
    "    ety = pd_entity.select()\n",
    "    ety = ety.where(and_(pd_entity.c.source_nid == orig_source_id, \n",
    "                         pd_entity.c.sname == entity_name))\n",
    "\n",
    "    rp = con.execute(ety)\n",
    "    \n",
    "    orig_entity = rp.first()\n",
    "    \n",
    "    if orig_entity is not None:\n",
    "        \n",
    "        orig_entity_nid = orig_entity.nid\n",
    "\n",
    "        wo = select([pd_workorder.c.nid,\n",
    "                     pd_workorder.c.start_time,\n",
    "                     pd_workorder.c.end_time,\n",
    "                     pd_workorder.c.record_count,\n",
    "                     pd_workorder.c.good_count,\n",
    "                     pd_workorder.c.bad_count,\n",
    "                     pd_workorder.c.ugly_count])\n",
    "\n",
    "        wo = wo.where(and_(pd_workorder.c.entity_nid == orig_entity_nid,\n",
    "                           pd_workorder.c.workorder_status == 'FINISHED'))\n",
    "        wo = wo.order_by(desc(pd_workorder.c.end_time))\n",
    "        wo = wo.limit(n) \n",
    "\n",
    "        rp = con.execute(wo)\n",
    "\n",
    "        r = rp.first()\n",
    "    \n",
    "    else:\n",
    "        r = None\n",
    "    \n",
    "    return r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_package_nodes(con, meta, bundle_nid):\n",
    "    \n",
    "    pd_prep_package = Table('podium_core.pd_prep_package', meta)\n",
    "    \n",
    "    s = pd_prep_package.select()\n",
    "    s = s.where(pd_prep_package.c.bundle_nid == bundle_nid)\n",
    "    \n",
    "    rp = con.execute(s)\n",
    "    \n",
    "    r = rp.fetchall()\n",
    "    \n",
    "    rp.close()\n",
    "    \n",
    "    return r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## podium_core Tables Used \n",
    "\n",
    "![ER](prep_dep_er.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_tables = ('pd_bundle',\n",
    "               'pd_bundle_gui_state',\n",
    "               'pd_prep_package',\n",
    "               'pd_entity',\n",
    "               'pd_source',\n",
    "               'pd_prepare_execution_workorder',\n",
    "               'pd_workorder'\n",
    "               )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establish connection to podium_core and fetch used tables metadata\n",
    "\n",
    "Enter the correct yaml file name (or stream) in the call to the `get_podium_cfg()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cfg = get_podium_cfg(open('pd_cfg.yaml', 'r'))\n",
    "\n",
    "con_cfg = cfg['pd_connect']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "con, meta = connect(con_cfg['user'], con_cfg['password'], con_cfg['db'], con_cfg['host'], con_cfg['port'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta.reflect(bind=con, schema='podium_core', only=prep_tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main get_bundle() Function\n",
    "\n",
    "This funcion is called with a single dataflow name (sname) and for each LOADER in the dataflow recurses backwards looking for dataflows that STORE that entity.\n",
    "\n",
    "If a dataflow is found the function self recurses.\n",
    "\n",
    "To prevent being caught in circular references, as each dataflow is visited it's name is stored in the wf_list, this list is checked each time the function is entered.\n",
    "\n",
    "The stop list \"stop_wf\" is a list of dataflow names that will also stop the recursion process.\n",
    "\n",
    "The Networkx DiGraph is built up as throughout the process adding nodes for Sources, Entities and Dataflows as they are first encountered.\n",
    "\n",
    "The node ids are the nid's from the related `pd_bundle` (Dataflow), `pd_source` (Source) and `pd_entity` (Entity) tables prefixes with the characters `b_`, `s_` and `e_` respectively.\n",
    "\n",
    "Edges are created between nodes to show the node relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bundle(con, meta, sname, world_graph, wf_list, stop_wf = []):\n",
    "    \"\"\"Build bundle dependency digraph\"\"\"\n",
    "    \n",
    "    # Check if dataflow is in stop list\n",
    "    if (sname.lower() in stop_wf):\n",
    "        print(f'Dataflow {sname} is in stop list\\n')\n",
    "        return\n",
    "    \n",
    "    bundle = get_bundle_id(con, meta, sname)\n",
    "    \n",
    "    #import pdb; pdb.set_trace()\n",
    "    \n",
    "    print(f'Current dataflow {bundle.sname} ({bundle.nid})')\n",
    "    \n",
    "    if bundle:\n",
    "        \n",
    "        bundle_nid = bundle.nid\n",
    "        bundle_description = bundle.description\n",
    "        bundle_sname = bundle.sname\n",
    "        \n",
    "        bundle_gui_state = get_bundle_gui_state(con, meta, bundle.bundle_gui_state_nid)\n",
    "        \n",
    "        if bundle_gui_state:\n",
    "            bundle_mod_dt = bundle_gui_state.modified_ttz\n",
    "            bundle_mod_by = bundle_gui_state.modifiedby\n",
    "            bundle_version = bundle_gui_state.version\n",
    "        else:\n",
    "            bundle_mod_dt = 'Unknown'\n",
    "            bundle_mod_by = 'Unknown'\n",
    "            bundle_version = 'Unknown'\n",
    "        \n",
    "        # To-do - check if output file for version already exists\n",
    "        #         if so then bypass\n",
    "        \n",
    "        bundle_exec = get_bundle_last_execution(con, meta, bundle_nid)\n",
    "        \n",
    "        if bundle_exec:\n",
    "            \n",
    "            exec_stats = []\n",
    "            \n",
    "            for i, r in enumerate(bundle_exec):\n",
    "                if i == 0:\n",
    "                    last_record_count = r.record_count\n",
    "                    last_start_time = r.start_time\n",
    "                    last_end_time = r.end_time\n",
    "                    \n",
    "                exec_stats.append(({'start_time': str(r.start_time), \n",
    "                                    'end_time': str(r.end_time),\n",
    "                                    'records': r.record_count}))\n",
    "                \n",
    "        else:\n",
    "            last_record_count = 0\n",
    "            last_start_time = ''\n",
    "            last_end_time = ''\n",
    "        \n",
    "        print(f'\\t{bundle_nid}, {bundle_description}, {bundle_sname} records {last_record_count}')\n",
    "        print(f'\\tModified by: {bundle_mod_by}, Modified Date: {bundle_mod_dt}, Version: {bundle_version}')\n",
    "        print(f'\\tLast Start: {last_start_time}, Last End: {last_end_time}')\n",
    "        \n",
    "    else:\n",
    "        print(f'Package: {sname}, not found')\n",
    "        return None\n",
    "    \n",
    "    # add bundle to \"world\" graph\n",
    "    bundle_node_key = f'b_{bundle_nid}'\n",
    "    \n",
    "    W.add_node(bundle_node_key,\n",
    "               nid=bundle_nid,\n",
    "               # sname=bundle_sname, \n",
    "               label=bundle_sname, \n",
    "               n_type='bundle',\n",
    "               shape=\"box\",\n",
    "               color=\"lightskyblue2\")\n",
    "     \n",
    "    # Add LOADER / STORE nodes\n",
    "    p = get_package_nodes(con, meta, bundle_nid)\n",
    "    \n",
    "    # Add LOADER and STORE nodes to graph\n",
    "    for n in p:\n",
    "        \n",
    "        id = n.nid\n",
    "        n_type = n.package_type\n",
    "     \n",
    "        if n_type in ('LOADER','STORE'):\n",
    "            \n",
    "            entity_id = n.entity_id\n",
    "            \n",
    "            entity_node_key = f'e_{entity_id}'\n",
    "            \n",
    "            if n_type == 'LOADER':\n",
    "                \n",
    "                l = get_entity_store(con, meta, entity_id)\n",
    "                \n",
    "                if len(l) == 0:\n",
    "                    print(f'No STORE found for {entity_id}')\n",
    "                else:\n",
    "                    for i, ldr in enumerate(l):\n",
    "                        print(f'{entity_id} ({i}) STORE by {ldr.sname}')\n",
    "                        \n",
    "                        if not (ldr.sname.lower() in wf_list):\n",
    "                            wf_list.append(ldr.sname.lower())\n",
    "                            get_bundle(con, meta, ldr.sname, world_graph, wf_list, stop_wf)\n",
    "            \n",
    "            if (not W.has_node(entity_node_key)):\n",
    "            \n",
    "                entity = get_entity(con, meta, entity_id)\n",
    "                \n",
    "                source_id = entity.source_nid\n",
    "                source = get_source(con, meta, source_id)\n",
    "                \n",
    "                W.add_node(entity_node_key,\n",
    "                           n_type='entity',\n",
    "                           nid=entity_id,\n",
    "                           snid=source_id,\n",
    "                           #sname=entity.sname,\n",
    "                           label=entity.sname,\n",
    "                           shape=\"ellipse\",\n",
    "                           color=\"lightgoldenrod1\")\n",
    "                \n",
    "                source_node_key = f's_{source_id}'\n",
    "                \n",
    "                if (not W.has_node(source_node_key)):\n",
    "                    \n",
    "                    W.add_node(source_node_key,\n",
    "                           n_type='source',\n",
    "                           nid=source_id,\n",
    "                           #sname=source.sname,\n",
    "                           label=source.sname,\n",
    "                           shape=\"octagon\",\n",
    "                           color=\"palegreen2\")\n",
    "                    \n",
    "                W.add_edge(source_node_key, \n",
    "                           entity_node_key,\n",
    "                           color=\"cyan\")\n",
    "                \n",
    "            else:\n",
    "                source_nid = W.node[entity_node_key]['snid']\n",
    "                source_node_key = f's_{source_nid}'\n",
    "                print(f\"Graph already has entity {entity_id}, {W.node[source_node_key]['label']}.{W.node[entity_node_key]['label']}\")\n",
    "\n",
    "            if (n_type == 'STORE'):                    \n",
    "                W.add_edge(bundle_node_key, entity_node_key)\n",
    "            elif (n_type == 'LOADER'):\n",
    "                W.add_edge(entity_node_key, bundle_node_key)\n",
    "            else:\n",
    "                print(f'ERROR {bundle_node_key}, {source_node_key}, {entity_node_key}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subg(g, sg, n_type):\n",
    "    \"\"\"Create a record type subgraph of the passsed node type\"\"\"\n",
    "    \n",
    "    \n",
    "    label_list = [g.node[n]['label'] for n in g.nodes if g.node[n]['n_type'] == n_type]\n",
    "    \n",
    "    label_list.sort(key=lambda x: x.lower())\n",
    "    \n",
    "    # Start subgraph and header record of number of lines\n",
    "    print(f'subgraph {sg} {{')\n",
    "    \n",
    "    print(f'r_{sg} [shape=record,label=\"{{')\n",
    "    print(f'{len(label_list)} {n_type}')\n",
    "    \n",
    "    for i, label in enumerate(label_list):\n",
    "        print(f'| {label}')\n",
    "    \n",
    "    # Close subgraph\n",
    "    print('}\"];}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_record_dot(g, output_file=None):\n",
    "    \n",
    "    print(\"digraph g {\")\n",
    "\n",
    "    subg(g, \"s1\", \"bundle\")\n",
    "    subg(g, \"s2\", \"source\")\n",
    "    subg(g, \"s3\", \"entity\")\n",
    "    \n",
    "    print('}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final dataflow name and \"stop list\"\n",
    "SEED = \"prod_stg_compsych_member_t\"\n",
    "\n",
    "# The stop list should be zero or more dataflow names that are stoppers in the\n",
    "# recursion. If a dataflow name in te STOPPER list is hit then the recursion will\n",
    "# stop at that point.\n",
    "STOPPERS = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing starting point, dataflow prod_stg_compsych_member_t\n",
      "Current dataflow prod_stg_Compsych_Member_T (6388)\n",
      "\t6388, , prod_stg_Compsych_Member_T records 731560\n",
      "\tModified by: je70, Modified Date: 2018-12-06 09:15:02.100000-05:00, Version: 4\n",
      "\tLast Start: 2018-12-06 11:47:29.549000-05:00, Last End: 2018-12-06 11:48:08.551000-05:00\n",
      "3954 (0) STORE by prod_copy_stg_compsych_sso_eligibility_t\n",
      "Current dataflow prod_copy_stg_compsych_sso_eligibility_t (6384)\n",
      "\t6384, , prod_copy_stg_compsych_sso_eligibility_t records 731560\n",
      "\tModified by: je70, Modified Date: 2018-12-06 09:14:29.965000-05:00, Version: 1\n",
      "\tLast Start: 2018-12-06 09:42:11.095000-05:00, Last End: 2018-12-06 09:42:44.479000-05:00\n",
      "3956 (0) STORE by prod_stg_compsych_sso_eligibility_cdc_t\n",
      "Current dataflow prod_stg_compsych_sso_eligibility_cdc_t (6352)\n",
      "\t6352, Constructs the stg_Compsych_Member_T History using the CDC transform., prod_stg_compsych_sso_eligibility_cdc_t records 1463120\n",
      "\tModified by: je70, Modified Date: 2018-12-06 09:13:45.707000-05:00, Version: 18\n",
      "\tLast Start: 2018-12-06 09:34:27.803000-05:00, Last End: 2018-12-06 09:38:40.641000-05:00\n",
      "No STORE found for 3943\n",
      "3954 (0) STORE by prod_copy_stg_compsych_sso_eligibility_t\n",
      "3954 (1) STORE by init_stg_Compsych_Member_T\n",
      "Current dataflow init_stg_Compsych_Member_T (6347)\n",
      "\t6347, , init_stg_Compsych_Member_T records 731560\n",
      "\tModified by: je70, Modified Date: 2018-12-06 09:12:57.482000-05:00, Version: 8\n",
      "\tLast Start: 2018-12-06 17:08:47.639000-05:00, Last End: 2018-12-06 17:13:31.924000-05:00\n",
      "Graph already has entity 3954, US_Derived.stg_compsych_sso_eligibility_t\n",
      "No STORE found for 3943\n",
      "Graph already has entity 3943, GWRRPT_PR.compsych_sso_eligibility_t\n",
      "Graph already has entity 3954, US_Derived.stg_compsych_sso_eligibility_t\n",
      "Graph already has entity 3956, US_Derived.stg_compsych_sso_eligibility_temp_t\n",
      "3954 (1) STORE by init_stg_Compsych_Member_T\n",
      "Graph already has entity 3954, US_Derived.stg_compsych_sso_eligibility_t\n",
      "11 nodes added to DiGraph\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    W = nx.DiGraph()\n",
    "\n",
    "    wf_list = []\n",
    "\n",
    "    wf_list.append(SEED.lower())\n",
    "    print(f'Processing starting point, dataflow {SEED}')\n",
    "    \n",
    "    get_bundle(con, meta, SEED, W, wf_list, STOPPERS)\n",
    "    \n",
    "\n",
    "    print(f'{len(W.nodes)} nodes added to DiGraph')\n",
    "\n",
    "    # Write output dot file\n",
    "    write_dot(W, f'{SEED}.dot')\n",
    "    \n",
    "    # Write output GraphML file\n",
    "    with open(f\"{SEED}.graphml\", \"wb\") as ofile:\n",
    "        nx.write_graphml(W, ofile)\n",
    "    \n",
    "    print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "digraph g {\n",
      "subgraph s1 {\n",
      "r_s1 [shape=record,label=\"{\n",
      "4 bundle\n",
      "| init_stg_Compsych_Member_T\n",
      "| prod_copy_stg_compsych_sso_eligibility_t\n",
      "| prod_stg_Compsych_Member_T\n",
      "| prod_stg_compsych_sso_eligibility_cdc_t\n",
      "}\"];}\n",
      "subgraph s2 {\n",
      "r_s2 [shape=record,label=\"{\n",
      "2 source\n",
      "| GWRRPT_PR\n",
      "| US_Derived\n",
      "}\"];}\n",
      "subgraph s3 {\n",
      "r_s3 [shape=record,label=\"{\n",
      "5 entity\n",
      "| compsych_sso_eligibility_t\n",
      "| full_stg_Compsych_Member_Audit_T\n",
      "| stg_Compsych_Member_T\n",
      "| stg_compsych_sso_eligibility_t\n",
      "| stg_compsych_sso_eligibility_temp_t\n",
      "}\"];}\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "write_record_dot(W, f'{SEED}_record.dot')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
